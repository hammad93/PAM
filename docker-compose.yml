services:
  llm:
    build:
      context: ./docker/llm
    ports:
        - "10000:10000"
    volumes:
        - ./symlink:/data
    restart: unless-stopped
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
        - /var/lib/docker/volumes/open-webui/_data:/app/backend/data
    restart: unless-stopped
    environment:
        - OFFLINE_MODE=True
    networks:
        - open-webui-internal

networks:
  open-webui-internal:
    name: open-webui-internal-network
    driver: bridge
    internal: true